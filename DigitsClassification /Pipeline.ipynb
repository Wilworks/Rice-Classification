{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf852608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722022a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MNIST image files into tensors of 4D. No of images, height, width and colour channels\n",
    "transform = transforms.ToTensor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b634920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root = 'cnn_data',train=True,download=True,transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0934bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(root='cnn_data',train=False,download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55ce2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,batch_size=10,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8a65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,3,1)\n",
    "        self.conv2 = nn.Conv2d(6,16,3,1)\n",
    "        #Fully Connected layer\n",
    "        self.fc1 = nn.Linear(400,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    #Forward Propagation\n",
    "    def forward(self,X):\n",
    "        #First pass\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X,2,2)\n",
    "        #self Pass\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X,2,2)\n",
    "\n",
    "        #Review to flatten\n",
    "        X = X.view(-1,16*5*5)\n",
    "\n",
    "        #Fully Connected Layer\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc3(X))\n",
    "\n",
    "        return F.log_softmax(X,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8539825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Instance\n",
    "torch.manual_seed(41)\n",
    "model = ConvolutionalNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d72422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e845a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 600 Loss: 2.2946114540100098\n",
      "Epoch: 1 Batch: 1200 Loss: 2.2969970703125\n",
      "Epoch: 1 Batch: 1800 Loss: 2.2978458404541016\n",
      "Epoch: 1 Batch: 2400 Loss: 2.2971012592315674\n",
      "Epoch: 1 Batch: 3000 Loss: 2.300535202026367\n",
      "Epoch: 1 Batch: 3600 Loss: 2.294329881668091\n",
      "Epoch: 1 Batch: 4200 Loss: 2.3096816539764404\n",
      "Epoch: 1 Batch: 4800 Loss: 2.300119400024414\n",
      "Epoch: 1 Batch: 5400 Loss: 2.2951743602752686\n",
      "Epoch: 1 Batch: 6000 Loss: 2.3093061447143555\n",
      "Epoch 1 completed. Train Loss: 2.3026, Test Loss: 2.3022\n",
      "Epoch: 2 Batch: 600 Loss: 2.304964542388916\n",
      "Epoch: 2 Batch: 1200 Loss: 2.315521717071533\n",
      "Epoch: 2 Batch: 1800 Loss: 2.2952239513397217\n",
      "Epoch: 2 Batch: 2400 Loss: 2.3124935626983643\n",
      "Epoch: 2 Batch: 3000 Loss: 2.308764934539795\n",
      "Epoch: 2 Batch: 3600 Loss: 2.3129725456237793\n",
      "Epoch: 2 Batch: 4200 Loss: 2.3105077743530273\n",
      "Epoch: 2 Batch: 4800 Loss: 2.2981550693511963\n",
      "Epoch: 2 Batch: 5400 Loss: 2.2996578216552734\n",
      "Epoch: 2 Batch: 6000 Loss: 2.300868272781372\n",
      "Epoch 2 completed. Train Loss: 2.3022, Test Loss: 2.3017\n",
      "Epoch: 3 Batch: 600 Loss: 2.290653944015503\n",
      "Epoch: 3 Batch: 1200 Loss: 2.297956705093384\n",
      "Epoch: 3 Batch: 1800 Loss: 2.310715675354004\n",
      "Epoch: 3 Batch: 2400 Loss: 2.2958016395568848\n",
      "Epoch: 3 Batch: 3000 Loss: 2.302274465560913\n",
      "Epoch: 3 Batch: 3600 Loss: 2.3018081188201904\n",
      "Epoch: 3 Batch: 4200 Loss: 2.3049771785736084\n",
      "Epoch: 3 Batch: 4800 Loss: 2.301684856414795\n",
      "Epoch: 3 Batch: 5400 Loss: 2.313141345977783\n",
      "Epoch: 3 Batch: 6000 Loss: 2.3099920749664307\n",
      "Epoch 3 completed. Train Loss: 2.3017, Test Loss: 2.3013\n",
      "Epoch: 4 Batch: 600 Loss: 2.311957359313965\n",
      "Epoch: 4 Batch: 1200 Loss: 2.2949001789093018\n",
      "Epoch: 4 Batch: 1800 Loss: 2.2968735694885254\n",
      "Epoch: 4 Batch: 2400 Loss: 2.3061535358428955\n",
      "Epoch: 4 Batch: 3000 Loss: 2.30863356590271\n",
      "Epoch: 4 Batch: 3600 Loss: 2.307234287261963\n",
      "Epoch: 4 Batch: 4200 Loss: 2.2960238456726074\n",
      "Epoch: 4 Batch: 4800 Loss: 2.286634922027588\n",
      "Epoch: 4 Batch: 5400 Loss: 2.2868127822875977\n",
      "Epoch: 4 Batch: 6000 Loss: 2.294961452484131\n",
      "Epoch 4 completed. Train Loss: 2.3012, Test Loss: 2.3008\n",
      "Epoch: 5 Batch: 600 Loss: 2.289759635925293\n",
      "Epoch: 5 Batch: 1200 Loss: 2.2914977073669434\n",
      "Epoch: 5 Batch: 1800 Loss: 2.308023452758789\n",
      "Epoch: 5 Batch: 2400 Loss: 2.296560287475586\n",
      "Epoch: 5 Batch: 3000 Loss: 2.3027656078338623\n",
      "Epoch: 5 Batch: 3600 Loss: 2.300549030303955\n",
      "Epoch: 5 Batch: 4200 Loss: 2.3086321353912354\n",
      "Epoch: 5 Batch: 4800 Loss: 2.291274070739746\n",
      "Epoch: 5 Batch: 5400 Loss: 2.307598352432251\n",
      "Epoch: 5 Batch: 6000 Loss: 2.292645215988159\n",
      "Epoch 5 completed. Train Loss: 2.3008, Test Loss: 2.3003\n",
      "Epoch: 6 Batch: 600 Loss: 2.3013994693756104\n",
      "Epoch: 6 Batch: 1200 Loss: 2.298401117324829\n",
      "Epoch: 6 Batch: 1800 Loss: 2.293940544128418\n",
      "Epoch: 6 Batch: 2400 Loss: 2.303274393081665\n",
      "Epoch: 6 Batch: 3000 Loss: 2.3124678134918213\n",
      "Epoch: 6 Batch: 3600 Loss: 2.2984557151794434\n",
      "Epoch: 6 Batch: 4200 Loss: 2.2971081733703613\n",
      "Epoch: 6 Batch: 4800 Loss: 2.306161403656006\n",
      "Epoch: 6 Batch: 5400 Loss: 2.2835257053375244\n",
      "Epoch: 6 Batch: 6000 Loss: 2.296649217605591\n",
      "Epoch 6 completed. Train Loss: 2.3003, Test Loss: 2.2998\n",
      "Epoch: 7 Batch: 600 Loss: 2.306138038635254\n",
      "Epoch: 7 Batch: 1200 Loss: 2.296037197113037\n",
      "Epoch: 7 Batch: 1800 Loss: 2.3085415363311768\n",
      "Epoch: 7 Batch: 2400 Loss: 2.3054893016815186\n",
      "Epoch: 7 Batch: 3000 Loss: 2.291304111480713\n",
      "Epoch: 7 Batch: 3600 Loss: 2.3037750720977783\n",
      "Epoch: 7 Batch: 4200 Loss: 2.3018953800201416\n",
      "Epoch: 7 Batch: 4800 Loss: 2.3171420097351074\n",
      "Epoch: 7 Batch: 5400 Loss: 2.3036932945251465\n",
      "Epoch: 7 Batch: 6000 Loss: 2.3091681003570557\n",
      "Epoch 7 completed. Train Loss: 2.2998, Test Loss: 2.2994\n",
      "Epoch: 8 Batch: 600 Loss: 2.2914257049560547\n",
      "Epoch: 8 Batch: 1200 Loss: 2.2997374534606934\n",
      "Epoch: 8 Batch: 1800 Loss: 2.3085737228393555\n",
      "Epoch: 8 Batch: 2400 Loss: 2.3037829399108887\n",
      "Epoch: 8 Batch: 3000 Loss: 2.2886626720428467\n",
      "Epoch: 8 Batch: 3600 Loss: 2.31172513961792\n",
      "Epoch: 8 Batch: 4200 Loss: 2.2960457801818848\n",
      "Epoch: 8 Batch: 4800 Loss: 2.302391529083252\n",
      "Epoch: 8 Batch: 5400 Loss: 2.306755304336548\n",
      "Epoch: 8 Batch: 6000 Loss: 2.2957687377929688\n",
      "Epoch 8 completed. Train Loss: 2.2994, Test Loss: 2.2988\n",
      "Epoch: 9 Batch: 600 Loss: 2.30617094039917\n",
      "Epoch: 9 Batch: 1200 Loss: 2.2938389778137207\n",
      "Epoch: 9 Batch: 1800 Loss: 2.302598476409912\n",
      "Epoch: 9 Batch: 2400 Loss: 2.305560350418091\n",
      "Epoch: 9 Batch: 3000 Loss: 2.2948830127716064\n",
      "Epoch: 9 Batch: 3600 Loss: 2.292837381362915\n",
      "Epoch: 9 Batch: 4200 Loss: 2.297393560409546\n",
      "Epoch: 9 Batch: 4800 Loss: 2.290440320968628\n",
      "Epoch: 9 Batch: 5400 Loss: 2.308804988861084\n",
      "Epoch: 9 Batch: 6000 Loss: 2.3076319694519043\n",
      "Epoch 9 completed. Train Loss: 2.2989, Test Loss: 2.2983\n",
      "Epoch: 10 Batch: 600 Loss: 2.293086051940918\n",
      "Epoch: 10 Batch: 1200 Loss: 2.311643123626709\n",
      "Epoch: 10 Batch: 1800 Loss: 2.298992872238159\n",
      "Epoch: 10 Batch: 2400 Loss: 2.3059418201446533\n",
      "Epoch: 10 Batch: 3000 Loss: 2.2932028770446777\n",
      "Epoch: 10 Batch: 3600 Loss: 2.2938220500946045\n",
      "Epoch: 10 Batch: 4200 Loss: 2.3027243614196777\n",
      "Epoch: 10 Batch: 4800 Loss: 2.2939865589141846\n",
      "Epoch: 10 Batch: 5400 Loss: 2.293257713317871\n",
      "Epoch: 10 Batch: 6000 Loss: 2.3024535179138184\n",
      "Epoch 10 completed. Train Loss: 2.2983, Test Loss: 2.2978\n",
      "Epoch: 11 Batch: 600 Loss: 2.2963638305664062\n",
      "Epoch: 11 Batch: 1200 Loss: 2.287386655807495\n",
      "Epoch: 11 Batch: 1800 Loss: 2.2886550426483154\n",
      "Epoch: 11 Batch: 2400 Loss: 2.3092527389526367\n",
      "Epoch: 11 Batch: 3000 Loss: 2.2916293144226074\n",
      "Epoch: 11 Batch: 3600 Loss: 2.2983040809631348\n",
      "Epoch: 11 Batch: 4200 Loss: 2.2936320304870605\n",
      "Epoch: 11 Batch: 4800 Loss: 2.3021953105926514\n",
      "Epoch: 11 Batch: 5400 Loss: 2.281813144683838\n",
      "Epoch: 11 Batch: 6000 Loss: 2.2946724891662598\n",
      "Epoch 11 completed. Train Loss: 2.2978, Test Loss: 2.2972\n",
      "Epoch: 12 Batch: 600 Loss: 2.2873690128326416\n",
      "Epoch: 12 Batch: 1200 Loss: 2.302694082260132\n",
      "Epoch: 12 Batch: 1800 Loss: 2.292656898498535\n",
      "Epoch: 12 Batch: 2400 Loss: 2.303647518157959\n",
      "Epoch: 12 Batch: 3000 Loss: 2.298906087875366\n",
      "Epoch: 12 Batch: 3600 Loss: 2.3075613975524902\n",
      "Epoch: 12 Batch: 4200 Loss: 2.3051445484161377\n",
      "Epoch: 12 Batch: 4800 Loss: 2.3038079738616943\n",
      "Epoch: 12 Batch: 5400 Loss: 2.2961926460266113\n",
      "Epoch: 12 Batch: 6000 Loss: 2.2924916744232178\n",
      "Epoch 12 completed. Train Loss: 2.2973, Test Loss: 2.2966\n",
      "Epoch: 13 Batch: 600 Loss: 2.299429416656494\n",
      "Epoch: 13 Batch: 1200 Loss: 2.2976603507995605\n",
      "Epoch: 13 Batch: 1800 Loss: 2.3048086166381836\n",
      "Epoch: 13 Batch: 2400 Loss: 2.3008193969726562\n",
      "Epoch: 13 Batch: 3000 Loss: 2.2888405323028564\n",
      "Epoch: 13 Batch: 3600 Loss: 2.308701515197754\n",
      "Epoch: 13 Batch: 4200 Loss: 2.309971332550049\n",
      "Epoch: 13 Batch: 4800 Loss: 2.2941231727600098\n",
      "Epoch: 13 Batch: 5400 Loss: 2.2950072288513184\n",
      "Epoch: 13 Batch: 6000 Loss: 2.2881178855895996\n",
      "Epoch 13 completed. Train Loss: 2.2967, Test Loss: 2.2960\n",
      "Epoch: 14 Batch: 600 Loss: 2.294543504714966\n",
      "Epoch: 14 Batch: 1200 Loss: 2.294572353363037\n",
      "Epoch: 14 Batch: 1800 Loss: 2.3088676929473877\n",
      "Epoch: 14 Batch: 2400 Loss: 2.276888608932495\n",
      "Epoch: 14 Batch: 3000 Loss: 2.3018715381622314\n",
      "Epoch: 14 Batch: 3600 Loss: 2.2963333129882812\n",
      "Epoch: 14 Batch: 4200 Loss: 2.301180362701416\n",
      "Epoch: 14 Batch: 4800 Loss: 2.290797710418701\n",
      "Epoch: 14 Batch: 5400 Loss: 2.2932164669036865\n",
      "Epoch: 14 Batch: 6000 Loss: 2.3057639598846436\n",
      "Epoch 14 completed. Train Loss: 2.2961, Test Loss: 2.2954\n",
      "Epoch: 15 Batch: 600 Loss: 2.2825920581817627\n",
      "Epoch: 15 Batch: 1200 Loss: 2.2914557456970215\n",
      "Epoch: 15 Batch: 1800 Loss: 2.305232524871826\n",
      "Epoch: 15 Batch: 2400 Loss: 2.3106751441955566\n",
      "Epoch: 15 Batch: 3000 Loss: 2.3006701469421387\n",
      "Epoch: 15 Batch: 3600 Loss: 2.3077406883239746\n",
      "Epoch: 15 Batch: 4200 Loss: 2.284034252166748\n",
      "Epoch: 15 Batch: 4800 Loss: 2.305752754211426\n",
      "Epoch: 15 Batch: 5400 Loss: 2.298891067504883\n",
      "Epoch: 15 Batch: 6000 Loss: 2.3034915924072266\n",
      "Epoch 15 completed. Train Loss: 2.2954, Test Loss: 2.2947\n",
      "Epoch: 16 Batch: 600 Loss: 2.292292356491089\n",
      "Epoch: 16 Batch: 1200 Loss: 2.2996127605438232\n",
      "Epoch: 16 Batch: 1800 Loss: 2.2928621768951416\n",
      "Epoch: 16 Batch: 2400 Loss: 2.301621675491333\n",
      "Epoch: 16 Batch: 3000 Loss: 2.303485631942749\n",
      "Epoch: 16 Batch: 3600 Loss: 2.287757158279419\n",
      "Epoch: 16 Batch: 4200 Loss: 2.299687623977661\n",
      "Epoch: 16 Batch: 4800 Loss: 2.284334182739258\n",
      "Epoch: 16 Batch: 5400 Loss: 2.3007524013519287\n",
      "Epoch: 16 Batch: 6000 Loss: 2.2980856895446777\n",
      "Epoch 16 completed. Train Loss: 2.2948, Test Loss: 2.2940\n",
      "Epoch: 17 Batch: 600 Loss: 2.2993931770324707\n",
      "Epoch: 17 Batch: 1200 Loss: 2.300065517425537\n",
      "Epoch: 17 Batch: 1800 Loss: 2.306328058242798\n",
      "Epoch: 17 Batch: 2400 Loss: 2.2951247692108154\n",
      "Epoch: 17 Batch: 3000 Loss: 2.297468423843384\n",
      "Epoch: 17 Batch: 3600 Loss: 2.292045831680298\n",
      "Epoch: 17 Batch: 4200 Loss: 2.293462038040161\n",
      "Epoch: 17 Batch: 4800 Loss: 2.294213056564331\n",
      "Epoch: 17 Batch: 5400 Loss: 2.300891876220703\n",
      "Epoch: 17 Batch: 6000 Loss: 2.2913193702697754\n",
      "Epoch 17 completed. Train Loss: 2.2941, Test Loss: 2.2933\n",
      "Epoch: 18 Batch: 600 Loss: 2.297205924987793\n",
      "Epoch: 18 Batch: 1200 Loss: 2.2985994815826416\n",
      "Epoch: 18 Batch: 1800 Loss: 2.291213274002075\n",
      "Epoch: 18 Batch: 2400 Loss: 2.3094265460968018\n",
      "Epoch: 18 Batch: 3000 Loss: 2.2813119888305664\n",
      "Epoch: 18 Batch: 3600 Loss: 2.301307201385498\n",
      "Epoch: 18 Batch: 4200 Loss: 2.310948610305786\n",
      "Epoch: 18 Batch: 4800 Loss: 2.2992844581604004\n",
      "Epoch: 18 Batch: 5400 Loss: 2.298699140548706\n",
      "Epoch: 18 Batch: 6000 Loss: 2.2765986919403076\n",
      "Epoch 18 completed. Train Loss: 2.2934, Test Loss: 2.2926\n",
      "Epoch: 19 Batch: 600 Loss: 2.303539514541626\n",
      "Epoch: 19 Batch: 1200 Loss: 2.2888877391815186\n",
      "Epoch: 19 Batch: 1800 Loss: 2.2946090698242188\n",
      "Epoch: 19 Batch: 2400 Loss: 2.2998743057250977\n",
      "Epoch: 19 Batch: 3000 Loss: 2.299314022064209\n",
      "Epoch: 19 Batch: 3600 Loss: 2.2840561866760254\n",
      "Epoch: 19 Batch: 4200 Loss: 2.2924728393554688\n",
      "Epoch: 19 Batch: 4800 Loss: 2.3030128479003906\n",
      "Epoch: 19 Batch: 5400 Loss: 2.303631067276001\n",
      "Epoch: 19 Batch: 6000 Loss: 2.291721820831299\n",
      "Epoch 19 completed. Train Loss: 2.2926, Test Loss: 2.2918\n",
      "Epoch: 20 Batch: 600 Loss: 2.297947645187378\n",
      "Epoch: 20 Batch: 1200 Loss: 2.2816481590270996\n",
      "Epoch: 20 Batch: 1800 Loss: 2.295319080352783\n",
      "Epoch: 20 Batch: 2400 Loss: 2.283797264099121\n",
      "Epoch: 20 Batch: 3000 Loss: 2.286729097366333\n",
      "Epoch: 20 Batch: 3600 Loss: 2.2961182594299316\n",
      "Epoch: 20 Batch: 4200 Loss: 2.3002724647521973\n",
      "Epoch: 20 Batch: 4800 Loss: 2.2931299209594727\n",
      "Epoch: 20 Batch: 5400 Loss: 2.291766405105591\n",
      "Epoch: 20 Batch: 6000 Loss: 2.2912955284118652\n",
      "Epoch 20 completed. Train Loss: 2.2918, Test Loss: 2.2910\n",
      "Epoch: 21 Batch: 600 Loss: 2.3062613010406494\n",
      "Epoch: 21 Batch: 1200 Loss: 2.288639545440674\n",
      "Epoch: 21 Batch: 1800 Loss: 2.3070592880249023\n",
      "Epoch: 21 Batch: 2400 Loss: 2.2782273292541504\n",
      "Epoch: 21 Batch: 3000 Loss: 2.305938482284546\n",
      "Epoch: 21 Batch: 3600 Loss: 2.295397996902466\n",
      "Epoch: 21 Batch: 4200 Loss: 2.290501832962036\n",
      "Epoch: 21 Batch: 4800 Loss: 2.2816383838653564\n",
      "Epoch: 21 Batch: 5400 Loss: 2.2902162075042725\n",
      "Epoch: 21 Batch: 6000 Loss: 2.2888617515563965\n",
      "Epoch 21 completed. Train Loss: 2.2910, Test Loss: 2.2901\n",
      "Epoch: 22 Batch: 600 Loss: 2.2867560386657715\n",
      "Epoch: 22 Batch: 1200 Loss: 2.2814831733703613\n",
      "Epoch: 22 Batch: 1800 Loss: 2.292215347290039\n",
      "Epoch: 22 Batch: 2400 Loss: 2.2973663806915283\n",
      "Epoch: 22 Batch: 3000 Loss: 2.2828381061553955\n",
      "Epoch: 22 Batch: 3600 Loss: 2.288381814956665\n",
      "Epoch: 22 Batch: 4200 Loss: 2.2824549674987793\n",
      "Epoch: 22 Batch: 4800 Loss: 2.2860796451568604\n",
      "Epoch: 22 Batch: 5400 Loss: 2.2641520500183105\n",
      "Epoch: 22 Batch: 6000 Loss: 2.2849228382110596\n",
      "Epoch 22 completed. Train Loss: 2.2902, Test Loss: 2.2892\n",
      "Epoch: 23 Batch: 600 Loss: 2.3023083209991455\n",
      "Epoch: 23 Batch: 1200 Loss: 2.2942705154418945\n",
      "Epoch: 23 Batch: 1800 Loss: 2.2850394248962402\n",
      "Epoch: 23 Batch: 2400 Loss: 2.2848598957061768\n",
      "Epoch: 23 Batch: 3000 Loss: 2.299624443054199\n",
      "Epoch: 23 Batch: 3600 Loss: 2.2851109504699707\n",
      "Epoch: 23 Batch: 4200 Loss: 2.3102221488952637\n",
      "Epoch: 23 Batch: 4800 Loss: 2.298039674758911\n",
      "Epoch: 23 Batch: 5400 Loss: 2.3099822998046875\n",
      "Epoch: 23 Batch: 6000 Loss: 2.2684123516082764\n",
      "Epoch 23 completed. Train Loss: 2.2893, Test Loss: 2.2883\n",
      "Epoch: 24 Batch: 600 Loss: 2.291882038116455\n",
      "Epoch: 24 Batch: 1200 Loss: 2.290597438812256\n",
      "Epoch: 24 Batch: 1800 Loss: 2.2964932918548584\n",
      "Epoch: 24 Batch: 2400 Loss: 2.2936840057373047\n",
      "Epoch: 24 Batch: 3000 Loss: 2.3022451400756836\n",
      "Epoch: 24 Batch: 3600 Loss: 2.284223794937134\n",
      "Epoch: 24 Batch: 4200 Loss: 2.289153575897217\n",
      "Epoch: 24 Batch: 4800 Loss: 2.296020746231079\n",
      "Epoch: 24 Batch: 5400 Loss: 2.286320209503174\n",
      "Epoch: 24 Batch: 6000 Loss: 2.280872344970703\n",
      "Epoch 24 completed. Train Loss: 2.2883, Test Loss: 2.2873\n",
      "Epoch: 25 Batch: 600 Loss: 2.294647693634033\n",
      "Epoch: 25 Batch: 1200 Loss: 2.301218271255493\n",
      "Epoch: 25 Batch: 1800 Loss: 2.283909320831299\n",
      "Epoch: 25 Batch: 2400 Loss: 2.299107789993286\n",
      "Epoch: 25 Batch: 3000 Loss: 2.2821192741394043\n",
      "Epoch: 25 Batch: 3600 Loss: 2.289252519607544\n",
      "Epoch: 25 Batch: 4200 Loss: 2.276128053665161\n",
      "Epoch: 25 Batch: 4800 Loss: 2.2852988243103027\n",
      "Epoch: 25 Batch: 5400 Loss: 2.263054370880127\n",
      "Epoch: 25 Batch: 6000 Loss: 2.2876298427581787\n",
      "Epoch 25 completed. Train Loss: 2.2874, Test Loss: 2.2863\n",
      "Epoch: 26 Batch: 600 Loss: 2.290731191635132\n",
      "Epoch: 26 Batch: 1200 Loss: 2.292872428894043\n",
      "Epoch: 26 Batch: 1800 Loss: 2.285482883453369\n",
      "Epoch: 26 Batch: 2400 Loss: 2.2910964488983154\n",
      "Epoch: 26 Batch: 3000 Loss: 2.2807202339172363\n",
      "Epoch: 26 Batch: 3600 Loss: 2.2894911766052246\n",
      "Epoch: 26 Batch: 4200 Loss: 2.306985378265381\n",
      "Epoch: 26 Batch: 4800 Loss: 2.279560089111328\n",
      "Epoch: 26 Batch: 5400 Loss: 2.293799877166748\n",
      "Epoch: 26 Batch: 6000 Loss: 2.274247646331787\n",
      "Epoch 26 completed. Train Loss: 2.2863, Test Loss: 2.2852\n",
      "Epoch: 27 Batch: 600 Loss: 2.2984936237335205\n",
      "Epoch: 27 Batch: 1200 Loss: 2.286062002182007\n",
      "Epoch: 27 Batch: 1800 Loss: 2.2708053588867188\n",
      "Epoch: 27 Batch: 2400 Loss: 2.2855937480926514\n",
      "Epoch: 27 Batch: 3000 Loss: 2.280890464782715\n",
      "Epoch: 27 Batch: 3600 Loss: 2.270864486694336\n",
      "Epoch: 27 Batch: 4200 Loss: 2.2798097133636475\n",
      "Epoch: 27 Batch: 4800 Loss: 2.2951931953430176\n",
      "Epoch: 27 Batch: 5400 Loss: 2.302924394607544\n",
      "Epoch: 27 Batch: 6000 Loss: 2.276214361190796\n",
      "Epoch 27 completed. Train Loss: 2.2853, Test Loss: 2.2841\n",
      "Epoch: 28 Batch: 600 Loss: 2.299349784851074\n",
      "Epoch: 28 Batch: 1200 Loss: 2.2784013748168945\n",
      "Epoch: 28 Batch: 1800 Loss: 2.295523166656494\n",
      "Epoch: 28 Batch: 2400 Loss: 2.279883861541748\n",
      "Epoch: 28 Batch: 3000 Loss: 2.284872531890869\n",
      "Epoch: 28 Batch: 3600 Loss: 2.283613681793213\n",
      "Epoch: 28 Batch: 4200 Loss: 2.2939467430114746\n",
      "Epoch: 28 Batch: 4800 Loss: 2.272496461868286\n",
      "Epoch: 28 Batch: 5400 Loss: 2.283402919769287\n",
      "Epoch: 28 Batch: 6000 Loss: 2.283653736114502\n",
      "Epoch 28 completed. Train Loss: 2.2841, Test Loss: 2.2829\n",
      "Epoch: 29 Batch: 600 Loss: 2.2960147857666016\n",
      "Epoch: 29 Batch: 1200 Loss: 2.2841992378234863\n",
      "Epoch: 29 Batch: 1800 Loss: 2.2708942890167236\n",
      "Epoch: 29 Batch: 2400 Loss: 2.2952497005462646\n",
      "Epoch: 29 Batch: 3000 Loss: 2.272200584411621\n",
      "Epoch: 29 Batch: 3600 Loss: 2.283740282058716\n",
      "Epoch: 29 Batch: 4200 Loss: 2.2813198566436768\n",
      "Epoch: 29 Batch: 4800 Loss: 2.272376298904419\n",
      "Epoch: 29 Batch: 5400 Loss: 2.2699875831604004\n",
      "Epoch: 29 Batch: 6000 Loss: 2.278754711151123\n",
      "Epoch 29 completed. Train Loss: 2.2829, Test Loss: 2.2816\n",
      "Epoch: 30 Batch: 600 Loss: 2.286320447921753\n",
      "Epoch: 30 Batch: 1200 Loss: 2.2715718746185303\n",
      "Epoch: 30 Batch: 1800 Loss: 2.2561097145080566\n",
      "Epoch: 30 Batch: 2400 Loss: 2.289306163787842\n",
      "Epoch: 30 Batch: 3000 Loss: 2.2677817344665527\n",
      "Epoch: 30 Batch: 3600 Loss: 2.261242389678955\n",
      "Epoch: 30 Batch: 4200 Loss: 2.2780814170837402\n",
      "Epoch: 30 Batch: 4800 Loss: 2.260521650314331\n",
      "Epoch: 30 Batch: 5400 Loss: 2.288451671600342\n",
      "Epoch: 30 Batch: 6000 Loss: 2.276620864868164\n",
      "Epoch 30 completed. Train Loss: 2.2816, Test Loss: 2.2802\n",
      "\n",
      "Training Time: 10.05 Minutes!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# ... (Assuming model, criterion, optimizer, train_loader, test_loader are defined)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Trackers\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = [] # Total correct per epoch\n",
    "test_correct = []  # Total correct per epoch\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    epoch_train_loss = 0 # New accumulator for training loss\n",
    "\n",
    "    # --- TRAIN ---\n",
    "    model.train() # Set model to training mode\n",
    "    for b, (X_train, Y_train) in enumerate(train_loader):\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, Y_train)\n",
    "        \n",
    "        # 2. Accumulate loss and accuracy\n",
    "        epoch_train_loss += loss.item() # Add batch loss\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        trn_corr += (predicted == Y_train).sum().item() # Accumulate correct predictions\n",
    "\n",
    "        # 3. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch progress (optional and kept from original logic)\n",
    "        if (b + 1) % 600 == 0:\n",
    "            print(f'Epoch: {i+1} Batch: {b+1} Loss: {loss.item()}')\n",
    "    \n",
    "    # 4. APPEND EPOCH-LEVEL TRAINING METRICS \n",
    "    train_losses.append(epoch_train_loss / len(train_loader)) # Average loss for the epoch\n",
    "    train_correct.append(trn_corr) # Total correct for the epoch\n",
    "\n",
    "    # --- TEST ---\n",
    "    tst_corr = 0\n",
    "    epoch_test_loss = 0 # New accumulator for test loss\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for X_test, Y_test in test_loader: # Removed 'b' as it's not strictly needed here\n",
    "            y_val = model(X_test)\n",
    "            \n",
    "            # 1. Accumulate loss and accuracy\n",
    "            test_loss = criterion(y_val, Y_test)\n",
    "            epoch_test_loss += test_loss.item() # Accumulate test batch loss\n",
    "            \n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == Y_test).sum().item()\n",
    "            \n",
    "    # 2. APPEND EPOCH-LEVEL TEST METRICS \n",
    "    test_losses.append(epoch_test_loss / len(test_loader)) # Average loss for the epoch\n",
    "    test_correct.append(tst_corr) # Total correct for the epoch\n",
    "    \n",
    "    # Optional: Print epoch summary\n",
    "    print(f'Epoch {i+1} completed. Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'\\nTraining Time: {total/60:.2f} Minutes!') # Use .2f for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
